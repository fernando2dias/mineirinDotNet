{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fernando2dias/mineirinDotNet/blob/main/Revis%C3%A3o_An%C3%A1lise_L%C3%A9xica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrcy-pZfKUNu"
      },
      "outputs": [],
      "source": [
        "%%writefile codigo.x\n",
        "init a\n",
        "init b\n",
        "init c\n",
        "\n",
        "a = 5\n",
        "b = 3\n",
        "c = \"12\"\n",
        "\n",
        "result = a + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD8fzJC-KUNw"
      },
      "outputs": [],
      "source": [
        "## lendo o código fonte\n",
        "\n",
        "arquivo = open('codigo.x','r')\n",
        "for l in arquivo.readlines():\n",
        "    l = l.replace('\\n','') # remove a quebra de linha\n",
        "    print(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eph9gCxKKUNw"
      },
      "outputs": [],
      "source": [
        "## iniciando a definição dos tokens que serão a saída da análise léxica\n",
        "\n",
        "T_KEYWORDS_INIT = \"<keyword init>\"\n",
        "T_OP = \"<op %s>\"\n",
        "T_INT = \"<int %s>\"\n",
        "T_STRING = \"<string %s>\"\n",
        "T_IDENTIF = \"<id %s>\"\n",
        "\n",
        "arquivo = open('codigo.x','r')\n",
        "for l in arquivo.readlines():\n",
        "    l = l.replace('\\n','') # remove a quebra de linha\n",
        "    if len(l) == 0:\n",
        "        continue\n",
        "    \n",
        "    tokens = []\n",
        "    for token in l.split():\n",
        "        tokens.append(token)\n",
        "    print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60BqeLOTKUNx"
      },
      "outputs": [],
      "source": [
        "## definindo o afd principal para processar e categorizar os tokens\n",
        "\n",
        "T_KEYWORDS_INIT = \"<keyword init>\"\n",
        "T_OP = \"<op %s>\"\n",
        "T_INT = \"<int %s>\"\n",
        "T_STRING = \"<string %s>\"\n",
        "T_IDENTIF = \"<id %s>\"\n",
        "\n",
        "def afd_principal(token):\n",
        "    \n",
        "    if token == \"init\":\n",
        "        return T_KEYWORDS_INIT\n",
        "    elif token == \"=\":\n",
        "        return T_OP % token\n",
        "\n",
        "    return None\n",
        "    \n",
        "arquivo = open('codigo.x','r')\n",
        "for l in arquivo.readlines():\n",
        "    l = l.replace('\\n','') # remove a quebra de linha\n",
        "    tokens = []\n",
        "    for token in l.split():\n",
        "        tokens.append(afd_principal(token))\n",
        "            \n",
        "    print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5hHd60bKUNx"
      },
      "outputs": [],
      "source": [
        "## definindo uma mensagem de erro para tokens que não puderem ser processados\n",
        "\n",
        "T_KEYWORDS_INIT = \"<keyword init>\"\n",
        "T_OP = \"<op %s>\"\n",
        "T_INT = \"<int %s>\"\n",
        "T_STRING = \"<string %s>\"\n",
        "T_IDENTIF = \"<id %s>\"\n",
        "\n",
        "class LexicalAnalysisError(Exception):\n",
        "    pass\n",
        "\n",
        "def afd_principal(token):\n",
        "    \n",
        "    if token == \"init\":\n",
        "        return T_KEYWORDS_INIT\n",
        "    elif token == \"=\":\n",
        "        return T_OP % token\n",
        "    else:\n",
        "        raise LexicalAnalysisError('Valor \"%s\" inesperado' % token)\n",
        "    return None\n",
        "    \n",
        "arquivo = open('codigo.x','r')\n",
        "ln = 1\n",
        "for l in arquivo.readlines():\n",
        "    l = l.replace('\\n','') # remove a quebra de linha\n",
        "    tokens = []\n",
        "    for token in l.split():\n",
        "        try:\n",
        "            tokens.append(afd_principal(token))\n",
        "        except LexicalAnalysisError as e:\n",
        "            print(tokens)\n",
        "            print(str(e) + \" na posição %i da linha %i\" % (l.index(token), ln))\n",
        "            raise\n",
        "    ln += 1\n",
        "\n",
        "    print(tokens)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}